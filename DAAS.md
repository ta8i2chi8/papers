# Discretization-Aware Architecture Search

## abstract
ニューラル・アーキテクチャ・サーチ(NAS)の探索コストは、ウェイトシェアリング法によって大きく削減されている。これらの方法では、すべてのエッジと演算を含むスーパーネットワークを最適化し、弱い候補を切り捨てる離散化によって最適なサブネットワークを決定します。しかし、この離散化処理は、操作やエッジに対して行われるため、精度が低く、最終的なアーキテクチャの品質は保証されない。本論文では、離散化を考慮したアーキテクチャ探索(DA2S)を提案する。その核となるアイデアは、スーパーネットワークを望ましいトポロジーの構成に向けて押し進めるための損失項を追加することで、離散化によってもたらされる精度の低下を大幅に軽減することである。標準的な画像分類ベンチマークを用いた実験では、特に、これまで研究されてこなかった不均衡なターゲットネットワーク構成において、本アプローチの優位性が実証された。

## 気づき
### DARTSは効率面では優れているが，最適化されたスーパーネットワークと、 サンプリングされたサブネットワークの間のギャップが問題となることが考えられる。
1. セル数の違いが「深さのギャップ」を引き起こす
2. DARTSでは、候補となる演算やエッジを重み付き和で組み合わせ、強い重みを持つ一定数の候補を保存し、それ以外の候補を破棄します。しかし、捨てられた重みが相対的に小さいという保証はありません。もしそうでなければ、この離散化プロセスは、各セルに対する神経応答に大きな不正確さをもたらす可能性があります。
### 2.について
* 特に，(i)廃棄された候補が中程度の重みを持っている場合、(ii)刈り込まれたエッジの数がスーパーネットワークのそれに比べて相対的に少ない場合、十分に最適化されたスーパーネットワークが必ずしも高品質のサブネットワークを生成するとは限らないという原因となる。
* 上記の問題を軽減するために、我々は離散化を考慮したアーキテクチャ探索（DA2S）を提案する。DA2Sの主なアイデアは、損失関数に追加項を導入することで、スーパーネットワークのアーキテクチャ・パラメータが探索プロセス中に望ましい構成に向かって徐々に押し出されるようにすることである。具体的には、システムのエントロピーを最小化することは、システム内の要素（重み）の疎性と離散化を最大化することにつながるという特性に基づいて、新しい損失項をエントロピー関数に定式化します。エントロピーの目的は、各重みが0か1のいずれかに近づくように強制することであり、1の数は目的の構成によって決定されます。これにより、重みが0に近い候補を削除する離散化プロセスで、大きな精度の損失が発生しないようにします。アーキテクチャのパラメータに対して微分可能なエントロピー関数は、SGD最適化のためのシステムに自由にプラグインすることができます。DARTSの効率的な改良版であるPC-DARTS[10]を用いて、2つの標準的な画像分類ベンチマーク、すなわちCIFAR10とImageNetで実験を行った。PC-DARTSには2つのアーキテクチャ・パラメータが存在し、それぞれエッジ内の演算とノードに結合するエッジを制御し、異なる損失項を持つ可能性があることに注意してください。我々は、これまでに研究されたことのない様々な構成（すなわち、各ノードの保存されたエッジの数が互いに異なる）を評価します。各探索プロセスが終了した時点で、スーパーネットワークは離散化に適した形に収束し、離散化プロセスによる精度の低下は、エントロピー損失なしで報告されたものよりもはるかに小さい。その結果、探索されたアーキテクチャは、どのような構成であっても、より高い性能を安定して発揮することができ、その優位性は、構成が不均衡になるほど大きくなり、オリジナルの探索方法では「離散化ギャップ」が大きくなる。