# Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild

## １．アブスト
我々は、微分可能なアーキテクチャ探索(DARTS)と呼ばれる現在人気のあるアルゴリズムを用いてこの設定を具体化しましたが、このアルゴリズムは、異なるタスク間で転送されている間、しばしば満足のいく性能を発揮しません。この精度の低下は、検索にはスーパーネットワークを使用し、再学習にはサブネットワークを使用するという方式に起因するものであると主張します。これらのステージの特性の違いにより、最適化のギャップが大きくなり、結果的にアーキテクチャのパラメータがスーパーネットワークに「過剰にフィット」することになりました。このギャップを緩和するために、我々は、探索段階でネットワークの深さを徐々に増加させるプログレッシブな方法を提示し、プログレッシブDARTS（P-DARTS）アルゴリズムを導き出しました。P-DARTSは探索コストを削減し(1台のGPUで7時間)、プロキシデータセット(CIFAR10)といくつかのターゲット問題(ImageNet分類、COCO検出、3つのReIDベンチマーク)の両方で性能向上を達成しました。

## ２．イントロ
* 不安定性の問題は、探索段階ではスーパーネットワークをプロキシデータセットに適合させるが、再学習段階では最適なサブネットワークを同じデータセットまたは異なるタスクに実際に適用することに起因する。
* P-DARTSの探索プロセスは複数のステージに分かれており、各ステージの終わりにはスーパーネットワークの深さが増していく。
* 上の手法には，２つの課題あり。
1. より深いスーパーネットワークで検索を行うと、より大きな計算量が必要となる。　→（ネットワークの深さを増やす際に候補数（演算数）を減らす、検索空間近似を提案）
2. 深いスーパーネットワークを最適化すると、勾配が不安定になる可能性があるため、探索アルゴリズムは、勾配の急激な減少方向に陥ることが多い学習不要の演算子であるskip-connectに大きく偏っている。その結果、発見されたアーキテクチャの学習能力が低下する。　→（(i)operationレベルのDropoutを導入して，skip-connectの優位性を緩和し、(ii)最終的なサブネットワークを決定する際のskip-connectの出現を正則化する。）

## ４．提案手法
### 新しい点
1. 学習を複数ステージに分け，ステージ毎にセルの数を増やして，trainとvalidのギャップを解消する。
2. skip-connectに頼る傾向があったので，探索空間正則化を提案する。（skip-connectの数を制限）
### メモ
* 典型的な最適化ギャップは、オペレーションのプルーニングプロセスの不整合によるものである。これは、スーパーネットワークの目的が、すべての候補操作のネットワーク重みωSとアーキテクチャパラメータαを共同で最適化することであるのに対し、ターゲットネットワークのトレーニングの目的が、選択されたいくつかの操作のネットワーク重みωEを最適化することだけであるためである。
* 最適化ギャップの中でも、ネットワークの深さが異なることによるギャップは、性能低下の主な原因の一つです。この問題を解決するために，我々は探索空間近似法を用いて，探索深度を段階的に増加させることを提案する．また、ネットワーク幅（特徴マップのチャンネル数）の不一致も、大規模で複雑なデータセット上でアーキテクチャを検索する際の性能に関わる重要な要因であり、我々は検索幅を段階的に増加させることでこの問題に対処する。具体的には、DARTSのアーキテクチャ探索は、8セルのスーパーネットワーク上で行われ、発見されたアーキテクチャは、20セル(CIFAR10)または14セル(ImageNet)のネットワーク上で評価されます。浅いネットワークと深いネットワークの振る舞いにはかなりの違いがあり（Ioffe and Szegedy 2015; Srivastava et al. 2015; He et al. 2016），これは探索過程で発見したアーキテクチャが必ずしも評価に最適なものではないことを意味する．我々はこれを検索と評価の間の深さのギャップと名付ける。これを検証するために、DARTSの探索プロセスを複数回実行したところ、発見されたアーキテクチャの通常のセルは、深い接続ではなく浅い接続を維持する傾向があることがわかりました。すなわち、探索アルゴリズムは、中間ノード間のカスケード接続ではなく、入力ノードに接続されたエッジを維持することを好みます。これは、浅いネットワークの方が探索過程での勾配降下が速いことが多いためです。しかし、このような特性は、深いネットワークがより良い性能を発揮する傾向があるという常識と矛盾する（Simonyan and Zisserman 2015; Szegedy et al.2015; He et al.2016; Huang et al.2017）。そこで我々は、探索プロセス中にネットワークの深さを漸進的に増加させ、探索終了時にスーパーネットワークの深さが評価に用いられたネットワーク構成に十分に近くなるような戦略で、深さのギャップを埋めることを提案する。ここで、探索深度を目標レベルまで直接増加させるのではなく、漸進的な方法を採用しているのは、浅いネットワークで探索することで、演算候補に関して探索空間を縮小し、深いネットワークでの探索のリスクを軽減することを期待しているからである。
* 各skip-connect操作の後に操作レベルのDropoutを挿入することで、skip-connectを介した直線的な経路を部分的に「遮断」し、アルゴリズムが他の操作を探索しやすくしています。しかし、常にskip-connectを介した経路を遮断してしまうと、アルゴリズムは低い重みを割り当ててそれらをドロップしてしまい、最終的なパフォーマンスに悪影響を与えてしまいます。この矛盾を解決するために、各探索段階の学習過程でDropout率を徐々に低下させることで、最初はskip-connectを通る素直な経路を遮断し、他の操作のパラメータが十分に学習された後は同等に扱い、アルゴリズム自身に判断を委ねています。

## ５．結果
* DARTSよりも深い構造を発見する傾向がある。
* CIFAR100でのsearchが可能となった。（DARTSでは，skip-connectだらけの構造となり，使い物にならなかったらしい）
* 浅い探索ネットワーク（セルの数が少ない）で生成されたアーキテクチャは、浅い接続を維持することを好む。しかし，深い探索ネットワーク（セルの数が多い）では、発見されたアーキテクチャは、深い接続を持つセルになる。これは、深い探索ネットワークを最適化することが難しいため、アルゴリズムは最適なものを見つけるために、より多くのパスを探索しなければならず、その結果、より複雑で強力なアーキテクチャになるからである。