# DARTS
## memo
### 2-1 search space
* セルは２入力１出力  
    * CNNの場合，２入力は，前の２つの層のセルからの出力
    * RNNの場合，２入力は，現在の状態に対する入力と前の状態の隠れ状態
### 2-2 continuous relaxation and optimization
* 目標は、アーキテクチャと、すべての混合演算内の重み$w$（例えば、コンボリューションフィルタの重み）を共同で学習することである。
* DARTSは検証損失を最適化することを目的としていて，手法は勾配降下法を用いている。
* 2レベル最適化問題
    * アーキテクチャ探索の目的は，検証損失$L_{val}$を最小化する$\alpha$（アーキテクチャの選択に関するパラメータ）を見つけること。
    * それぞれのアーキテクチャの重み$w$は，訓練損失$L_{train}$の最小化によって得られる。
    * つまり，まとめると，まず検証データを使ってαを最適化し，そのαと訓練データで重み$ｗ$を最適化する。
### 2-3 approximate architecture gradient
* アーキテクチャの勾配を正確に評価することは難しいため，簡単な近似法を提案している。
$$\nabla_{\alpha}L_{val}(\omega^{*}(\alpha), \alpha)$$
$$\approx \nabla_{\alpha}L_{val}(\omega - \xi\nabla_{\omega}L_{train}(\omega,\alpha), \alpha)$$
* 上の式には２次の微分もでてくるため，そこについても1次に近似する式がある。
* $\xi=0$としたとき，2次微分は消えてしまう。この方法を1次近似と呼び，$\xi > 0$のときを２次近似と呼ぶ。1次近似では経験的に性能が低下するとわかっている。
### 2-4 deriving discrete architectures
* 離散型アーキテクチャの各ノードを形成するために，前のすべてのノードから集められたゼロではないすべてのoperation候補のうち，（別のノードから得られた）上位k個の最も強いoperationを保持する。
    * 上記でゼロ演算が除外されているのには2つの理由がある。第一に、既存のモデルと公平に比較するためには、ノードごとに正確にk個の非ゼロ着信エッジが必要である。第二に、ゼロ演算のロジットを増加させることは、結果として得られるノード表現のスケールに影響するだけであり、バッチ正規化の存在により最終的な分類結果には影響しないため、ゼロ演算の強度は過小決定される。
* operationの強さは，$\frac{\exp(\alpha_{o}^{(i, j)})}{\Sigma_{o^{'}}\exp(\alpha_{o^{'}}^{(i, j)})}$として定義される。
* 我々の派生アーキテクチャを既存の作品のものと比較可能にするために、畳み込みセルにはk = 2 、リカレントセルにはk = 1を使用。
### 3 experiment and results
CIFAR-10とPTBを対象とした実験は、アーキテクチャ探索とアーキテクチャ評価の2段階で構成されている。第一段階では、DARTSを用いてセル・アーキテクチャを検索し、その検証性能に基づいて最適なセルを決定します。第2段階では、これらのセルを用いてより大きなアーキテクチャを構築し、ゼロからトレーニングを行い、テストセットでのパフォーマンスを報告します。また、CIFAR-10とPTBで学習した最良のセルを、それぞれImageNetとWikiText-2(WT2)で評価することで、その移植性を調べる。
### 3-1 ARCHITECTURE SEARCH (cnn)
* Oには次のような演算が含まれる。  
3 3と5 5 sepConv、3 3と5 5 dilConv、3 3 max pool、3 3 avg pool、Identity、Zero
* すべての演算はストライド1
* 畳み込みセルは、N = 7ノードで構成され、そのうち出力ノードは、すべての中間ノード（入力ノードを除く）の深さ方向の連結として定義される。
* セルkの第1ノードと第2ノードは、それぞれセルk-2とセルk-1の出力に等しく、必要に応じて1*1の畳み込みが挿入される。
* ネットワークの総深度の1/3と2/3に位置するセルはリダクションセルであり、入力ノードに隣接する演算はすべてストライド2である。