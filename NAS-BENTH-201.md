# NAS-BENCH-201: EXTENDING THE SCOPE OF RE- PRODUCIBLE NEURAL ARCHITECTURE SEARCH

## abstract
ニューラル・アーキテクチャ・サーチ（NAS）は、ここ数年で非常に多くのアプリケーションで画期的な成功を収めています。しかし、そろそろ一歩下がって、NASの分野における良い面と悪い面を分析する時期に来ているのかもしれません。様々なアルゴリズムが、異なる検索空間の下でアーキテクチャを検索します。これらの検索されたアーキテクチャは、ハイパーパラメータ、データ増強、正則化などの異なるセットアップを使用してトレーニングされます。このことは、様々なNASアルゴリズムの性能を比較する際に、比較可能性の問題を引き起こす。NAS-Bench-101は、この問題を軽減することに成功している。本研究では、NAS-Bench-101の拡張版として、異なる探索空間、複数のデータセットでの結果、およびより多くの診断情報を持つNAS-Bench-201を提案する。NAS-Bench-201は、固定の探索空間を持ち、ほぼすべての最新のNASアルゴリズムに対して統一的なベンチマークを提供します。NAS-Bench-201の検索空間は、最も一般的なセルベースの検索アルゴリズムで使用されているものにヒントを得て設計されており、セルは有向非環状グラフとして表現されています。ここでの各エッジは、事前に定義された操作セットから選択された操作に関連付けられています。すべてのNASアルゴリズムに適用できるように、NAS-Bench-201で定義された探索空間には、4つのノードと5つの関連する操作オプションによって生成されるすべての可能なアーキテクチャが含まれており、その結果、合計で15,625個のニューラルセル候補が得られました。3つのデータセットについて、同じ設定を用いたトレーニングログと、各アーキテクチャ候補の性能を提供しています。これにより、研究者は、選択したアーキテクチャに対する不必要な繰り返しのトレーニングを避け、探索アルゴリズム自体にのみ集中することができます。また、各アーキテクチャのトレーニング時間が短縮されることで、ほとんどのNASアルゴリズムの効率が大きく改善され、より多くの研究者にとって計算コストに優しいNASコミュニティが実現します。我々は、NASアルゴリズムの新しい設計にインスピレーションを与えることができる、きめ細かな損失や精度などの追加診断情報を提供します。さらに、提案するNAS-Bench201を多面的に分析し、最近の10個のNASアルゴリズムをベンチマークして、その適用性を検証した。

## introduction
### 多くのアルゴリズムにおいて，設定が異なっている。
1. 異なる探索空間が利用されている
2. データ拡張，正則化，スケジューラ，ハイパーパラメータが異なる
3. データの分割割合（train, val)
* 上記問題が比較可能性の問題を生み出す

### 本研究の価値
1. NASBench-201は、すべてのセルベースNAS手法を含む最新のNASアルゴリズムの統一的なベンチマークを提供。研究者は、探索アーキテクチャの面倒なハイパーパラメータの調整を避けながら、ロバストな探索アルゴリズムの設計に集中することができる。
2. 各アーキテクチャの完全なトレーニングログを提供。選択された各アーキテクチャの不要な反復的なトレーニング手順を回避することができ，研究者はNASの本質、すなわち探索アルゴリズムに的を絞ることができる。
3. 複数のデータセットに対する各アーキテクチャの結果を提供。
4. NAS-Bench-201では、提案された探索空間の体系的な分析を行っている。また、強化学習(RL)ベースの手法、進化戦略(ES)ベースの手法、微分可能ベースの手法など、最近の先進的なNASアルゴリズム10種類を評価している。

## NAS-BENCH-201
### architectures in the search space
* 構造(channel)  
image(3) -> conv(16) -> BN -> cell1(16) -> reduction -> cell2(32) -> reduction -> cell3(64) -> GAP 
* operations  
Zero, skipconnections, 1*1conv, 3*3conv, 3*3pooling

### datasets
* cifar10, cifar100, imagenet-16-120
* split(train, val, test)

### architecture performance
我々のNAS-Bench-201では、過去の文献に従ってハイパーパラメータとトレーニング戦略を設定しています。 表1に示すように、各アーキテクチャを同じ戦略でトレーニングします。簡略化のため、モデルをトレーニングするためのすべてのハイパーパラメータをセットHとし、使用するハイパーパラメータの値を示すためにH†を使用しています。具体的には，クロスエントロピー損失を用いた Nesterov Momentum SGD により，各アーキテクチャを合計 200 エポック スで学習します．重みの減衰を0.0005とし、コサインアニーリングで学習率を0.1から0に減衰させます（Loshchilov & Hutter, 2017）。異なるデータセットで同じH†を使用していますが、画像の解像度のために若干異なるデータ増強を除いています。CIFARでは，確率0.5のランダムフリップ，各境界に4ピクセルのパディングを施した32×32パッチのランダムクロップ，RGBチャンネルに対する正規化を行っている．ImageNet-16-120では、同様の手法を用いていますが、16×16のパッチをランダムにクロップし、それぞれの境界に2ピクセルのパディングを施しています。全てのデータセットでH†を使用する以外に、CIFAR-10では異なるハイパーパラメータセットH‡を使用しています。これはH†と似ていますが、トレーニングエポックの総数は12です。このようにして、バンディットベースのアルゴリズム(Falkner et al., 2018; Li et al., 2018)に、短いトレーニングバジェットの使用について、より多くの選択肢を提供することができました。

