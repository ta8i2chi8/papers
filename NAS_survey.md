# NAS survey
## 1. Early NAS
### 1-1 構造
事前に定義した操作の集合と多くの構造を得るためのコントローラーをもつ。  
例：NAS-RL, MetaQNN, Large-scale Evolution  
### 1-2 アルゴリズム  
1. RNNをによるコントローラを使って候補アーキテクチャ(child model)を抽出  
2. child modelを重みが収束するまで学習してその性能を評価する  
3. その性能指標をguiding signalとして，より良いアーキテクチャを抽出するようRNNコントローラを学習する  
4. 以上を繰り返す
### 1-3 課題点
* Global search space  
ニューラル・アーキテクチャの必要なコンポーネントをすべて検索するsearch strategyを用いる必要がある。つまり，非常に大きなsearch spaceの中で最適なアーキテクチャを見つける必要がある。それにより，探索コストも高くなる。
* Discrate search strategy  
異なるアーキテクチャの違いを限られた基本操作のセットとみなす。つまり，ある操作を離散的に変更することでアーキテクチャを変更するというものである。それにより，勾配戦略を用いてアーキテクチャを素早く調整することができない。
* Search from scratch  
最終的なアーキテクチャが生成されるまで，モデルをゼロから構築する。そのため，既存のニューラル・アーキテクチャの設計経験を利用することができない。
* Fully trained
各候補のアーキテクチャを１からトレーニングする必要がある。  
後続のネットワークと前のネットワーク構造は，同じ段階のアーキテクチャと同様に類似している。したがって，１から学習していては，この関係が十分に生かされていないことは明らかである。  
また，私たちが必要となるのは，候補となるアーキテクチャの相対的な性能ランキングであるため，収束するまで学習する必要があるかどうかの検討に値する問題である。
### 1-4 その他
* NASNetでは，分岐の演算として和演算と結合演算で実験を行っているが，和演算のほうが優れていることがわかっている。そのため，これ以降多くの研究で，異なる分岐演算の間で得られた特徴マップの接続方法として，和演算を採用している。  
## 2. Existing NAS
### 2-1 Early NASの問題点に対する解決
* Modular search space  
Global search spaceに対応して，Modular search spaceはアーキテクチャを複数の異なるタイプのモジュール（セルまたはブロックベース）のスタック（積み重ね）として扱う。そのため，探索タスクの対象はモジュールに限定されるので簡略化される。  
セルベースのsearch spaceは，セルを積み重ねることで調整ができるため，異なるデータセットのタスクに移行することができる。（グローバル探索では無理）
    * reduction sellについて  
    効率化するために，unit operationに置き換えられることがある。（Avg pooling, Max pooling, 
    Conv3*3(stride2))
    * セルベースのセル  
    広くて浅いセルの方が収束しやすく探索もしやすいが，汎化性能が低いことが理論的，実験的に証明されている。
    * FPNAS：探索プロセスをbi-level最適化問題として扱うことで探索コストを低減
    * FBNet：マクロアーキテクチャを固定し，複数層を持つブロックを探索する。各ブロックは異なる層構造を持ち，ブロックも異なるものとすることができる。
* Continuous search strategy  
descrete search strategyに対応して，Continuous search strategyはネットワークパラメータのように勾配最適化できるようにアーキテクチャの構造パラメータを連続的に緩和する。
    * DARTS：もともと離散的な探索戦略を継続的に緩和することで，勾配を利用してアーキテクチャ探索空間を効率的に最適化する。
    * I-DARTS：ノード間のエッジにはsoftmaxがかけるのだが，DARTSの場合，ひとつのノードからのエッジをsoftmaxにかける。しかし，I-DARTSの場合は繋がっているすべてのエッジに対してsoftmaxをかける。
    * P-DARTS：DARTSの場合，計算資源の制限から探索段階では浅いセルスタックアーキテクチャを使用し，そして評価段階では，より高解像度のデータセットの処理を可能にするために，より多くの探索セルをスタックする。これにより，探索段階で使用するアーキテクチャは評価段階と異なり，浅い設計となっている。これに基づきP-DARTSでは，探索段階でネットワークの深さを徐々に増やしていく。
* Neural architecture recycling  
Search from scratchに対応して，Neural architecture recyclingは既存の人工的に設計された高性能のニューラルアーキテクチャを出発点として，NAS法を用いてネットワーク変換を行い，性能を向上させる。
    * EAS：エンコーダネットワークをメタコントローラとして使用し，既存のニューラルアーキテクチャの低次元表現を学習し，さらにNet2Netの複数のアクターネットワークを参照して，層レベルでニューラルネットアーキテクチャに対応する調整を行うかを決定する。
* Incomplete training  
fully trainingに対応して，incomplete trainingは候補のアーキテクチャ間の共有構造や性能予測を最大限に活用することで，相対的な性能ランキング化を高速化することを目的とする。これにより，すべての候補アーキテクチャを完全に訓練することによるリソースの消費を避けることができる。
### 2-2 ENAS(efficient NAS)
* すべてのchild modelsがネットワーク重みを共有するように強制する
    * これにより，child modelを収束させるまでの学習時間の無駄を回避

